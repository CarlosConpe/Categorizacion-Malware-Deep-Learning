{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWAnSz7mpE10"
   },
   "source": [
    "We import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "svRj36CkURK2"
   },
   "outputs": [],
   "source": [
    "#Machine learning:\n",
    "import tensorflow as tf\n",
    "#C math port\n",
    "import numpy as np\n",
    "#Random integer  generator\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#           GRADIENT BOOSTING MACHINE\n",
    "# ==============================================================================\n",
    "\n",
    "# Tratamiento de datos\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Gráficos\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocesado y modelado\n",
    "# ==============================================================================\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.inspection import permutation_importance\n",
    "import multiprocessing\n",
    "\n",
    "# Configuración warnings\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('once')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We import our abstract machine learning model class that we made earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "niuF0YP0HsWX"
   },
   "outputs": [],
   "source": [
    "#Abstract class for machine learning models. We will use this to inherit to specific models and override each method when inherited.\n",
    "class MachineLearningModel:\n",
    "  #CONSTRUCTOR: Currently does nothing besides storing the name we gave it\n",
    "  def __init__(self, * ,model_name, verbose):\n",
    "    self.name = model_name\n",
    "    self.verbose = verbose\n",
    "\n",
    "  #Method for compiling our machine learning model\n",
    "  def compile(self):\n",
    "    if self.verbose:\n",
    "      print(\"Model Compiled!\")\n",
    "\n",
    "  #Method for training our machine learning model. It will use the processed dataset\n",
    "  def train(self, data):\n",
    "    if self.verbose:\n",
    "      print(\"Model Trained!\")\n",
    "    self.training_time = 1000\n",
    "\n",
    "  #Method for evaluating our machine learning model. We use this to compute how accurate it is\n",
    "  def evaluate(self, samples):\n",
    "    for sample in samples:\n",
    "      print(sample)\n",
    "    self.precision = 0.9\n",
    "    self.loss = 0.1\n",
    "    if self.verbose:\n",
    "      print(\"Model Accuracy:\", self.precision)\n",
    "    self.mean_evaluation_time = 10\n",
    "\n",
    "  #Method to run the entire model. This is interface that would be used for any model. \n",
    "  def run_model(self, dataset, samples):\n",
    "    self.compile()\n",
    "    self.train(dataset)\n",
    "    self.evaluate(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PBzUHbmWY04"
   },
   "source": [
    "2. We build a neural network class that inherits from our abstract machine learning model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "gX1_GIWvUF4g"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(MachineLearningModel):\n",
    "  #Not inherited: Method to build neural network based on architecture specifications\n",
    "  def build(self):\n",
    "    layer_array = []\n",
    "    #We attach our input layer into our layer array:\n",
    "    layer_array.append(tf.keras.layers.Flatten(input_shape = self.network_architecture['input_size']))\n",
    "    #We attach our hidden layers into our layer array:\n",
    "    for layer in self.network_architecture['architecture']:\n",
    "      layer_array.append(tf.keras.layers.Dense(layer, activation = \"relu\"))\n",
    "    #We attach our output layer into our layer array\n",
    "    layer_array.append(tf.keras.layers.Dense(self.network_architecture['output_size']))\n",
    "    #We create a neural network model using the layer array specifications. self.model is our actual network\n",
    "    self.model = tf.keras.Sequential(layer_array)\n",
    "\n",
    "  #Constructor:\n",
    "  def __init__(self, *, architecture, input_size, output_size, epochs, name, verbose):\n",
    "    #We inherit from the machine learning model class:\n",
    "    super(NeuralNetwork, self).__init__(model_name = name, verbose = verbose)\n",
    "    #We create a dictionary/hash map with the properties of our neural network\n",
    "    self.network_architecture = {'architecture': architecture, \n",
    "                                 'input_size': input_size, \n",
    "                                 'output_size': output_size, \n",
    "                                 'epochs': epochs}\n",
    "    #We build our neural network based on the specifications\n",
    "    self.build()\n",
    "\n",
    "  #We compile the built neural network\n",
    "  def compile(self):\n",
    "    our_optimizer = 'adam'\n",
    "    our_metric = 'accuracy'\n",
    "    self.model.compile(optimizer = our_optimizer, loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), metrics = [our_metric])\n",
    "  \n",
    "  #We train our neural network with a given dataset\n",
    "  def train(self, *, training_data, training_labels):\n",
    "    self.model.fit(training_data, training_labels, epochs = self.network_architecture['epochs'])\n",
    "    \n",
    "  #We check how accurate our neural network is\n",
    "  def evaluate(self, samples):\n",
    "    testing_labels = samples['testing_labels']\n",
    "    testing_data = samples['testing_data']\n",
    "\n",
    "    probability_model = tf.keras.Sequential([self.model, tf.keras.layers.Softmax()])\n",
    "    predictions = probability_model.predict(testing_data)\n",
    "\n",
    "    correct_guesses = 0\n",
    "    for i in range(0,len(testing_labels)):\n",
    "      if testing_labels[i] == np.argmax(probability_model.predict(np.expand_dims(testing_data[i],0))):\n",
    "        correct_guesses += 1\n",
    "    self.accuracy = correct_guesses / len(testing_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "class SupportVectorMachine(MachineLearningModel):\n",
    "    \n",
    "    def __init__(self,name,verbose):\n",
    "        super(SupportVectorMachine, self).__init__(model_name = name, verbose = verbose)\n",
    "\n",
    "    \"\"\"\n",
    "    Build classifier\n",
    "    :param regularization: Regularization parameter. The strength of the regularization is inversely \n",
    "    proportional to C. Must be strictly positive.\n",
    "    :param kernel: Specifies the kernel type to be used in the algorithm.\n",
    "    :param degree: Degree of the polynomial kernel function (poly). Ignored by all other kernels.\n",
    "    :param gamma{scale, auto} : Kernel coefficient for rbf, poly and sigmoid.\n",
    "    \"\"\"     \n",
    "    def build(self, regularization=1.0, kernel='rbf', degree=3, gamma='scale'):\n",
    "        self.clf = SVC(C=regularization,kernel=kernel, degree=degree, gamma= gamma) \n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        # fit classifier to training set\n",
    "        self.clf.fit(X_train,y_train)\n",
    "    \n",
    "    def evaluate(self, samples):\n",
    "        y_predict = self.clf.predict(samples)\n",
    "        return y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing svm class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "Model accuracy score with default hyperparameters: 0.9827\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = './test_databases/pulsar_stars.csv'\n",
    "df = pd.read_csv(data)\n",
    "\n",
    "# remove leading spaces from column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# rename column names\n",
    "df.columns = ['IP Mean', 'IP Sd', 'IP Kurtosis', 'IP Skewness','DM-SNR Mean', 'DM-SNR Sd', 'DM-SNR Kurtosis', 'DM-SNR Skewness', 'target_class']\n",
    "\n",
    "\n",
    "# check distribution of target_class column\n",
    "df['target_class'].value_counts()\n",
    "\n",
    "X = df.drop(['target_class'], axis=1)\n",
    "y = df['target_class']\n",
    "\n",
    "# split X and y into training and testing sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "cols = X_train.columns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=[cols])\n",
    "X_test = pd.DataFrame(X_test, columns=[cols])\n",
    "\n",
    "svm = SupportVectorMachine(name=\"TESTING SVM\",verbose=False)\n",
    "svm.build()\n",
    "svm.train(X_train, y_train)\n",
    "train = svm.evaluate(X_test)\n",
    "\n",
    "print(train)\n",
    "\n",
    "print('Model accuracy score with default hyperparameters: {0:0.4f}'. format(accuracy_score(y_test, train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected array-like (array or non-string sequence), got None",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [158], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[39m# compute and print accuracy score\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[39mprint\u001B[39m(\u001B[39m'\u001B[39m\u001B[39mModel accuracy score with default hyperparameters: \u001B[39m\u001B[39m{0:0.4f}\u001B[39;00m\u001B[39m'\u001B[39m\u001B[39m.\u001B[39m \u001B[39mformat\u001B[39m(accuracy_score(y_test, y_pred)))\n",
      "File \u001B[1;32mc:\\Users\\contm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:211\u001B[0m, in \u001B[0;36maccuracy_score\u001B[1;34m(y_true, y_pred, normalize, sample_weight)\u001B[0m\n\u001B[0;32m    145\u001B[0m \u001B[39m\"\"\"Accuracy classification score.\u001B[39;00m\n\u001B[0;32m    146\u001B[0m \n\u001B[0;32m    147\u001B[0m \u001B[39mIn multilabel classification, this function computes subset accuracy:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[39m0.5\u001B[39;00m\n\u001B[0;32m    208\u001B[0m \u001B[39m\"\"\"\u001B[39;00m\n\u001B[0;32m    210\u001B[0m \u001B[39m# Compute accuracy for each possible representation\u001B[39;00m\n\u001B[1;32m--> 211\u001B[0m y_type, y_true, y_pred \u001B[39m=\u001B[39m _check_targets(y_true, y_pred)\n\u001B[0;32m    212\u001B[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001B[0;32m    213\u001B[0m \u001B[39mif\u001B[39;00m y_type\u001B[39m.\u001B[39mstartswith(\u001B[39m\"\u001B[39m\u001B[39mmultilabel\u001B[39m\u001B[39m\"\u001B[39m):\n",
      "File \u001B[1;32mc:\\Users\\contm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:86\u001B[0m, in \u001B[0;36m_check_targets\u001B[1;34m(y_true, y_pred)\u001B[0m\n\u001B[0;32m     84\u001B[0m check_consistent_length(y_true, y_pred)\n\u001B[0;32m     85\u001B[0m type_true \u001B[39m=\u001B[39m type_of_target(y_true, input_name\u001B[39m=\u001B[39m\u001B[39m\"\u001B[39m\u001B[39my_true\u001B[39m\u001B[39m\"\u001B[39m)\n\u001B[1;32m---> 86\u001B[0m type_pred \u001B[39m=\u001B[39m type_of_target(y_pred, input_name\u001B[39m=\u001B[39;49m\u001B[39m\"\u001B[39;49m\u001B[39my_pred\u001B[39;49m\u001B[39m\"\u001B[39;49m)\n\u001B[0;32m     88\u001B[0m y_type \u001B[39m=\u001B[39m {type_true, type_pred}\n\u001B[0;32m     89\u001B[0m \u001B[39mif\u001B[39;00m y_type \u001B[39m==\u001B[39m {\u001B[39m\"\u001B[39m\u001B[39mbinary\u001B[39m\u001B[39m\"\u001B[39m, \u001B[39m\"\u001B[39m\u001B[39mmulticlass\u001B[39m\u001B[39m\"\u001B[39m}:\n",
      "File \u001B[1;32mc:\\Users\\contm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\multiclass.py:278\u001B[0m, in \u001B[0;36mtype_of_target\u001B[1;34m(y, input_name)\u001B[0m\n\u001B[0;32m    273\u001B[0m valid \u001B[39m=\u001B[39m (\n\u001B[0;32m    274\u001B[0m     \u001B[39misinstance\u001B[39m(y, Sequence) \u001B[39mor\u001B[39;00m issparse(y) \u001B[39mor\u001B[39;00m \u001B[39mhasattr\u001B[39m(y, \u001B[39m\"\u001B[39m\u001B[39m__array__\u001B[39m\u001B[39m\"\u001B[39m)\n\u001B[0;32m    275\u001B[0m ) \u001B[39mand\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39misinstance\u001B[39m(y, \u001B[39mstr\u001B[39m)\n\u001B[0;32m    277\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m valid:\n\u001B[1;32m--> 278\u001B[0m     \u001B[39mraise\u001B[39;00m \u001B[39mValueError\u001B[39;00m(\n\u001B[0;32m    279\u001B[0m         \u001B[39m\"\u001B[39m\u001B[39mExpected array-like (array or non-string sequence), got \u001B[39m\u001B[39m%r\u001B[39;00m\u001B[39m\"\u001B[39m \u001B[39m%\u001B[39m y\n\u001B[0;32m    280\u001B[0m     )\n\u001B[0;32m    282\u001B[0m sparse_pandas \u001B[39m=\u001B[39m y\u001B[39m.\u001B[39m\u001B[39m__class__\u001B[39m\u001B[39m.\u001B[39m\u001B[39m__name__\u001B[39m \u001B[39min\u001B[39;00m [\u001B[39m\"\u001B[39m\u001B[39mSparseSeries\u001B[39m\u001B[39m\"\u001B[39m, \u001B[39m\"\u001B[39m\u001B[39mSparseArray\u001B[39m\u001B[39m\"\u001B[39m]\n\u001B[0;32m    283\u001B[0m \u001B[39mif\u001B[39;00m sparse_pandas:\n",
      "\u001B[1;31mValueError\u001B[0m: Expected array-like (array or non-string sequence), got None"
     ]
    }
   ],
   "source": [
    "# compute and print accuracy score\n",
    "print('Model accuracy score with default hyperparameters: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Machine Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingMachine(MachineLearningModel):\n",
    "    #Constructor\n",
    "    def __init__(self,name,verbose):\n",
    "        super(GradientBoostingMachine, self).__init__(model_name = name, verbose = verbose)\n",
    "\n",
    "    def build(self, n_estimators = [50, 100, 500, 1000], max_features = ['auto', 'sqrt', 'log2'], max_depth = [None, 1, 3, 5, 10, 20], subsample = [0.5, 1], learning_rate = [0.001, 0.01, 0.1]):\n",
    "        self.param_grid = {\n",
    "            'n_estimators'  : n_estimators,\n",
    "            'max_features'  : max_features,\n",
    "            'max_depth'     : max_depth,\n",
    "            'subsample'     : subsample,\n",
    "            'learning_rate' : learning_rate\n",
    "        }\n",
    "\n",
    "        self.grid = GridSearchCV(\n",
    "            estimator  = GradientBoostingClassifier(random_state=123),\n",
    "            param_grid = self.param_grid,\n",
    "            scoring    = 'accuracy',\n",
    "            n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "            cv         = RepeatedKFold(n_splits=3, n_repeats=1, random_state=123),\n",
    "            refit      = True,\n",
    "            verbose    = self.verbose,\n",
    "            return_train_score = True\n",
    "        )\n",
    "\n",
    "    #We compile the Gradient Boosting Machine\n",
    "    def compile(self):\n",
    "        self.build()\n",
    "\n",
    "    def train(self, training_data , training_labels):\n",
    "        self.grid.fit(X = training_data, y = training_labels)\n",
    "        # Resultados\n",
    "        # ==============================================================================\n",
    "        resultados = pd.DataFrame(self.grid.cv_results_)\n",
    "        resultados.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "            .drop(columns = 'params') \\\n",
    "            .sort_values('mean_test_score', ascending = False) \\\n",
    "            .head(4)\n",
    "\n",
    "        # Mejores hiperparámetros por validación cruzada\n",
    "        # ==============================================================================\n",
    "        print(\"----------------------------------------\")\n",
    "        print(\"Mejores hiperparámetros encontrados (cv)\")\n",
    "        print(\"----------------------------------------\")\n",
    "        print(self.grid.best_params_, \":\", self.grid.best_score_, self.grid.scoring)\n",
    "\n",
    "        self.modelo_final = self.grid.best_estimator_\n",
    "\n",
    "\n",
    "    def evaluate(self, samples):\n",
    "        # Error de test del modelo final\n",
    "        # ==============================================================================\n",
    "        y_test = samples['testing_labels']\n",
    "        x_test = samples['testing_data']\n",
    "\n",
    "        self.predicciones = self.modelo_final.predict(X = x_test)\n",
    "        #self.predicciones[:10]\n",
    "\n",
    "        self.mat_confusion = confusion_matrix(\n",
    "            y_true    = y_test,\n",
    "            y_pred    = self.predicciones\n",
    "        )\n",
    "\n",
    "        self.accuracy = accuracy_score(\n",
    "            y_true    = y_test,\n",
    "            y_pred    = self.predicciones,\n",
    "            normalize = True\n",
    "        )\n",
    "\n",
    "        print(\"Matriz de confusión\")\n",
    "        print(\"-------------------\")\n",
    "        print(self.mat_confusion)\n",
    "        print(\"\")\n",
    "        print(f\"El accuracy del test es: {100 * self.accuracy} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Our GBM Model With Car Sales from R Library Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. container::\n",
      "\n",
      "   ======== ===============\n",
      "   Carseats R Documentation\n",
      "   ======== ===============\n",
      "\n",
      "   .. rubric:: Sales of Child Car Seats\n",
      "      :name: sales-of-child-car-seats\n",
      "\n",
      "   .. rubric:: Description\n",
      "      :name: description\n",
      "\n",
      "   A simulated data set containing sales of child car seats at 400\n",
      "   different stores.\n",
      "\n",
      "   .. rubric:: Usage\n",
      "      :name: usage\n",
      "\n",
      "   ::\n",
      "\n",
      "      Carseats\n",
      "\n",
      "   .. rubric:: Format\n",
      "      :name: format\n",
      "\n",
      "   A data frame with 400 observations on the following 11 variables.\n",
      "\n",
      "   ``Sales``\n",
      "      Unit sales (in thousands) at each location\n",
      "\n",
      "   ``CompPrice``\n",
      "      Price charged by competitor at each location\n",
      "\n",
      "   ``Income``\n",
      "      Community income level (in thousands of dollars)\n",
      "\n",
      "   ``Advertising``\n",
      "      Local advertising budget for company at each location (in\n",
      "      thousands of dollars)\n",
      "\n",
      "   ``Population``\n",
      "      Population size in region (in thousands)\n",
      "\n",
      "   ``Price``\n",
      "      Price company charges for car seats at each site\n",
      "\n",
      "   ``ShelveLoc``\n",
      "      A factor with levels ``Bad``, ``Good`` and ``Medium`` indicating\n",
      "      the quality of the shelving location for the car seats at each\n",
      "      site\n",
      "\n",
      "   ``Age``\n",
      "      Average age of the local population\n",
      "\n",
      "   ``Education``\n",
      "      Education level at each location\n",
      "\n",
      "   ``Urban``\n",
      "      A factor with levels ``No`` and ``Yes`` to indicate whether the\n",
      "      store is in an urban or rural location\n",
      "\n",
      "   ``US``\n",
      "      A factor with levels ``No`` and ``Yes`` to indicate whether the\n",
      "      store is in the US or not\n",
      "\n",
      "   .. rubric:: Source\n",
      "      :name: source\n",
      "\n",
      "   Simulated data\n",
      "\n",
      "   .. rubric:: References\n",
      "      :name: references\n",
      "\n",
      "   James, G., Witten, D., Hastie, T., and Tibshirani, R. (2013) *An\n",
      "   Introduction to Statistical Learning with applications in R*,\n",
      "   https://www.statlearning.com, Springer-Verlag, New York\n",
      "\n",
      "   .. rubric:: Examples\n",
      "      :name: examples\n",
      "\n",
      "   ::\n",
      "\n",
      "      summary(Carseats)\n",
      "      lm.fit=lm(Sales~Advertising+Price,data=Carseats)\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 300 entries, 0 to 299\n",
      "Data columns (total 14 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   CompPrice         300 non-null    float64\n",
      " 1   Income            300 non-null    float64\n",
      " 2   Advertising       300 non-null    float64\n",
      " 3   Population        300 non-null    float64\n",
      " 4   Price             300 non-null    float64\n",
      " 5   Age               300 non-null    float64\n",
      " 6   Education         300 non-null    float64\n",
      " 7   ShelveLoc_Bad     300 non-null    float64\n",
      " 8   ShelveLoc_Good    300 non-null    float64\n",
      " 9   ShelveLoc_Medium  300 non-null    float64\n",
      " 10  Urban_No          300 non-null    float64\n",
      " 11  Urban_Yes         300 non-null    float64\n",
      " 12  US_No             300 non-null    float64\n",
      " 13  US_Yes            300 non-null    float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 32.9 KB\n",
      "----------------------------------------\n",
      "Mejores hiperparámetros encontrados (cv)\n",
      "----------------------------------------\n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'max_features': 'auto', 'n_estimators': 1000, 'subsample': 1} : 0.84 accuracy\n",
      "Matriz de confusión\n",
      "-------------------\n",
      "[[33 17]\n",
      " [ 6 44]]\n",
      "\n",
      "El accuracy del test es: 77.0 %\n"
     ]
    }
   ],
   "source": [
    "#Charge Dataset\n",
    "carseats = sm.datasets.get_rdataset(\"Carseats\", \"ISLR\")\n",
    "datos = carseats.data\n",
    "print(carseats.__doc__)\n",
    "\n",
    "#Configure the data for classification \n",
    "datos['ventas_altas'] = np.where(datos.Sales > 8, 0, 1) #Clasificar si venden mucho o poco\n",
    "# Una vez creada la nueva variable respuesta se descarta la original\n",
    "datos = datos.drop(columns = 'Sales')\n",
    "\n",
    "\n",
    "\n",
    "# DATA PREPROCESING\n",
    "\n",
    "# División de los datos en train y test\n",
    "# ==============================================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                        datos.drop(columns = 'ventas_altas'),\n",
    "                                        datos['ventas_altas'],\n",
    "                                        random_state = 123,\n",
    "                                    )\n",
    "\n",
    "# One-hot-encoding de las variables categóricas\n",
    "# ==============================================================================\n",
    "# Se identifica el nombre de las columnas numéricas y categóricas\n",
    "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.to_list()\n",
    "numeric_cols = X_train.select_dtypes(include=['float64', 'int']).columns.to_list()\n",
    "\n",
    "# Se aplica one-hot-encoding solo a las columnas categóricas\n",
    "preprocessor = ColumnTransformer(\n",
    "                    [('onehot', OneHotEncoder(handle_unknown='ignore'), cat_cols)],\n",
    "                    remainder='passthrough'\n",
    "               )\n",
    "\n",
    "# Una vez que se ha definido el objeto ColumnTransformer, con el método fit()\n",
    "# se aprenden las transformaciones con los datos de entrenamiento y se aplican a\n",
    "# los dos conjuntos con transform(). Ambas operaciones a la vez con fit_transform().\n",
    "X_train_prep = preprocessor.fit_transform(X_train)\n",
    "X_test_prep  = preprocessor.transform(X_test)\n",
    "\n",
    "# Convertir el output del ColumnTransformer en dataframe y añadir el nombre de las columnas\n",
    "# ==============================================================================\n",
    "# Nombre de todas las columnas\n",
    "encoded_cat = preprocessor.named_transformers_['onehot'].get_feature_names_out(cat_cols)\n",
    "labels = np.concatenate([numeric_cols, encoded_cat])\n",
    "\n",
    "# Conversión a dataframe\n",
    "X_train_prep = pd.DataFrame(X_train_prep, columns=labels)\n",
    "X_test_prep  = pd.DataFrame(X_test_prep, columns=labels)\n",
    "X_train_prep.info()\n",
    "\n",
    "\n",
    "gbm = GradientBoostingMachine(name=\"TESTING SVM\",verbose=False)\n",
    "gbm.build()\n",
    "gbm.train(X_train_prep, y_train)\n",
    "train = gbm.evaluate(X_test_prep,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSjYiGvTid7h"
   },
   "source": [
    "3. We create an ensamble class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAFcAyCRinuT"
   },
   "outputs": [],
   "source": [
    "#In the simplest form, an ensamble is an array of N machine learning models\n",
    "#The ensamble class should therefore operate as an array container class\n",
    "\n",
    "class Ensamble(MachineLearningModel):\n",
    "  def build(self, ensamble_size):\n",
    "    self.array = []\n",
    "    for i in range(0, ensamble_size):\n",
    "      model_id = '' + i\n",
    "      self.array.append(self.model_type)\n",
    "\n",
    "  def __init__(self, *, ensamble_size, model_type, name, verbose):\n",
    "      super(Ensamble, self).__init__(model_name = name, verbose = verbose)\n",
    "      self.size = ensamble_size\n",
    "      self.model_type = model_type\n",
    "      #self.build(ensamble_size)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryQNS-2Rmi0v"
   },
   "source": [
    "4. We create a data parser class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18YQGsERmkkQ"
   },
   "outputs": [],
   "source": [
    "#This class will process data and adapt it to something a given model can work with\n",
    "#VERY UNFINISHED!!! Currently has minimum requirements for neural network compatibility\n",
    "class DataParser:\n",
    "  #We store the dataset during the construction phase\n",
    "  def __init__(self, data_src):\n",
    "    self.data_src = data_src\n",
    "    self.neural_network_dataset = None\n",
    "    self.testing_samples = None\n",
    "\n",
    "#We will use this method to compute the number of unique labels. Currently not functional!!!\n",
    "  def get_unique_labels(self):\n",
    "    return 10\n",
    "\n",
    "  #Method to distribute and preprocess data\n",
    "  def preprocess(self, *, training, testing):\n",
    "    self.training_size = training\n",
    "    self.testing_size = testing\n",
    "    (self.training_data, self.training_labels), (self.testing_data, self.testing_labels) = self.data_src.load_data()\n",
    "    self.training_data = self.training_data / 255\n",
    "    self.testing_data = self.testing_data / 255\n",
    "    self.data_shape = self.training_data[0].shape\n",
    "    self.output_shape = self.get_unique_labels()\n",
    "\n",
    "  #Method to print dataset information\n",
    "  def print_dataset_info(self):\n",
    "    print(\"Dataset properties: \")\n",
    "    print(\"\\tSize: \", len(self.data_src[0]))\n",
    "    print(\"\\tParameters:\", len(self.data_src))\n",
    "\n",
    "  def generate_testing_samples(self, *, sample_size):\n",
    "    if self.testing_samples == None:\n",
    "      samples = {}\n",
    "      testing_labels = []\n",
    "      testing_data = []\n",
    "      for i in range(0, min(sample_size, len(self.testing_labels))):\n",
    "        random_index = randint(0, min(sample_size, len(self.testing_labels)))\n",
    "        testing_labels.append(self.testing_labels[random_index])\n",
    "        testing_data.append(self.testing_data[random_index])\n",
    "\n",
    "      testing_labels = np.array(testing_labels)\n",
    "      testing_data = np.array(testing_data)\n",
    "      samples['testing_labels'] = testing_labels\n",
    "      samples['testing_data'] = testing_data\n",
    "\n",
    "      self.testing_samples = samples\n",
    "      return self.testing_samples\n",
    "\n",
    "\n",
    "  #We convert our dataset into one compatible with a sequential neural network\n",
    "  def convert_to_neural_network(self):\n",
    "    if self.neural_network_dataset == None:\n",
    "      self.neural_network_dataset = self.data_src\n",
    "    return self.neural_network_dataset\n",
    "  \n",
    "  #Future implementations\n",
    "  def convert_to_cnn(self):\n",
    "    pass\n",
    "\n",
    "  def convert_to_kernel(self):\n",
    "    pass\n",
    "\n",
    "  def convert_to_svm(self):\n",
    "    pass\n",
    "\n",
    "  def convert_to_gradient_ascent(self):\n",
    "    pass\n",
    "  \n",
    "  def convert_to_gradient_descent(self):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKcThHFbmASR"
   },
   "source": [
    "5. We instantiate tests of our classes and see if they are storing information correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5kv-31rIIZFs",
    "outputId": "4df893c8-970a-4e10-f2e8-20394cd1f295"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test ensamble\n",
      "test neural network\n",
      "<__main__.NeuralNetwork object at 0x000002E2AD982920>\n"
     ]
    }
   ],
   "source": [
    "#We create a test neural network\n",
    "our_network = NeuralNetwork(\n",
    "    architecture = [3, 2, 4], \n",
    "    input_size = (8, 8), \n",
    "    output_size = 10, \n",
    "    epochs = 3, \n",
    "    name = \"test neural network\", \n",
    "    verbose = False)\n",
    "\n",
    "#We create a test ensamble of the type of our neural network\n",
    "our_ensamble = Ensamble(\n",
    "    ensamble_size = 10,\n",
    "    model_type = our_network,\n",
    "    name = \"test ensamble\",\n",
    "    verbose = False)\n",
    "\n",
    "#We check to see if our models are storing information correctly\n",
    "print(our_ensamble.name)\n",
    "print(our_network.name)\n",
    "print(our_ensamble.model_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfi97iA4Sthc"
   },
   "source": [
    "6. Now that we know the model classes are behaving correctly, we perform a test run of our data parser on a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RjVWv-4gStAe",
    "outputId": "55ae77e8-cbe4-42ae-f1e2-ca85329a2c5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2350 - accuracy: 0.9313\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1017 - accuracy: 0.9691\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0703 - accuracy: 0.9783\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Network accuracy:  99.0\n"
     ]
    }
   ],
   "source": [
    "#We create an instance of our data parser, we will use the MNIST dataset to test\n",
    "our_parser = DataParser(tf.keras.datasets.mnist)\n",
    "\n",
    "#Parser creates training and testing dataset from original dataset\n",
    "our_parser.preprocess(training = 0.7, testing = 0.3)\n",
    "\n",
    "#We create a neural network that is compatible with our testing dataset\n",
    "mnist_network = NeuralNetwork(\n",
    "    architecture = [128, 64], \n",
    "    input_size = our_parser.data_shape, \n",
    "    output_size = our_parser.output_shape, \n",
    "    epochs = 3, \n",
    "    name = \"MNIST neural network\", \n",
    "    verbose = False)\n",
    "\n",
    "#We compile our network according to the construction specifications\n",
    "mnist_network.compile()\n",
    "\n",
    "#We train our neural network with a dataset made for neural networks by our parser\n",
    "mnist_network.train(training_data = our_parser.training_data, training_labels = our_parser.training_labels)\n",
    "\n",
    "#We check how accurate our model is. If its around 90+% then it is very accurate\n",
    "mnist_network.evaluate(our_parser.generate_testing_samples(sample_size = 300))\n",
    "print(\"Network accuracy: \", 100* mnist_network.accuracy)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "cdab306a66d6ce7293b5fd3d2ec4cc052c1059f4e47e1e95c26be1fb0272e406"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
