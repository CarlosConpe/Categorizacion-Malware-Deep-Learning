{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We import the necessary libraries"
      ],
      "metadata": {
        "id": "hMAZll78TRwd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xfh5xNviNk2d"
      },
      "outputs": [],
      "source": [
        "#Machine learning:\n",
        "import tensorflow as tf\n",
        "#C math port\n",
        "import numpy as np\n",
        "#Random integer  generator\n",
        "from random import randint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ==============================================================================\n",
        "#           GRADIENT BOOSTING MACHINE\n",
        "# ==============================================================================\n",
        "\n",
        "# Data Treatment\n",
        "# ==============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Graphics\n",
        "# ==============================================================================\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Modeling and Preprocessing\n",
        "# ==============================================================================\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.inspection import permutation_importance\n",
        "import multiprocessing\n",
        "\n",
        "# Warnings configuration\n",
        "# ==============================================================================\n",
        "import warnings\n",
        "warnings.filterwarnings('once')\n",
        "\n"
      ],
      "metadata": {
        "id": "wuwhGun2r7Xe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abstract machine learning model"
      ],
      "metadata": {
        "id": "Wx14QOU1TVOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Abstract class for machine learning models. We will use this to inherit to specific models and override each method when inherited.\n",
        "class MachineLearningModel:\n",
        "  #CONSTRUCTOR: Currently does nothing besides storing the name we gave it\n",
        "  def __init__(self, * ,model_name, verbose):\n",
        "    self.name = model_name\n",
        "    self.verbose = verbose\n",
        "\n",
        "  #Method for compiling our machine learning model\n",
        "  def compile(self):\n",
        "    if self.verbose:\n",
        "      print(\"Model Compiled!\")\n",
        "\n",
        "  #Method for training our machine learning model. It will use the processed dataset\n",
        "  def train(self, data):\n",
        "    if self.verbose:\n",
        "      print(\"Model Trained!\")\n",
        "    self.training_time = 1000\n",
        "\n",
        "  #Method for evaluating our machine learning model. We use this to compute how accurate it is\n",
        "  def evaluate(self, samples):\n",
        "    for sample in samples:\n",
        "      print(sample)\n",
        "    self.precision = 0.9\n",
        "    self.loss = 0.1\n",
        "    if self.verbose:\n",
        "      print(\"Model Accuracy:\", self.precision)\n",
        "    self.mean_evaluation_time = 10\n",
        "\n",
        "  #Method to run the entire model. This is interface that would be used for any model. \n",
        "  def run_model(self, dataset, samples):\n",
        "    self.compile()\n",
        "    self.train(dataset)\n",
        "    self.evaluate(samples)"
      ],
      "metadata": {
        "id": "BuWtBVSqNt5V"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural network class"
      ],
      "metadata": {
        "id": "hYsJIFqJJe14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(MachineLearningModel):\n",
        "  #Not inherited: Method to build neural network based on architecture specifications\n",
        "  def build(self):\n",
        "    layer_array = []\n",
        "    #We attach our input layer into our layer array:\n",
        "    layer_array.append(tf.keras.layers.Flatten(input_shape = self.network_architecture['input_size']))\n",
        "    #We attach our hidden layers into our layer array:\n",
        "    for layer in self.network_architecture['architecture']:\n",
        "      layer_array.append(tf.keras.layers.Dense(layer, activation = \"relu\"))\n",
        "    #We attach our output layer into our layer array\n",
        "    layer_array.append(tf.keras.layers.Dense(self.network_architecture['output_size'], activation = \"sigmoid\"))\n",
        "    #We create a neural network model using the layer array specifications. self.model is our actual network\n",
        "    self.model = tf.keras.Sequential(layer_array)\n",
        "\n",
        "  #Constructor:\n",
        "  def __init__(self, *, architecture, input_size, output_size, epochs, name, verbose):\n",
        "    #We inherit from the machine learning model class:\n",
        "    super(NeuralNetwork, self).__init__(model_name = name, verbose = verbose)\n",
        "    #We create a dictionary/hash map with the properties of our neural network\n",
        "    self.network_architecture = {'architecture': architecture, \n",
        "                                 'input_size': input_size, \n",
        "                                 'output_size': output_size, \n",
        "                                 'epochs': epochs}\n",
        "    #We build our neural network based on the specifications\n",
        "    self.build()\n",
        "\n",
        "  #We compile the built neural network\n",
        "  def compile(self):\n",
        "    our_optimizer = 'adam'\n",
        "    our_metric = 'accuracy'\n",
        "    self.model.compile(optimizer = our_optimizer, loss = tf.keras.losses.BinaryCrossentropy(from_logits = False), metrics = [our_metric])\n",
        "  \n",
        "  #We train our neural network with a given dataset\n",
        "  def train(self, *, training_data, training_labels):\n",
        "    self.model.fit(training_data, training_labels, validation_split = 0.3, epochs = self.network_architecture['epochs'], shuffle = True)\n",
        "    \n",
        "  #We check how accurate our neural network is\n",
        "  def evaluate(self, samples):\n",
        "    #We grab the testing data so we can run our test\n",
        "    testing_labels = samples['testing_labels']\n",
        "    testing_data = samples['testing_data']\n",
        "\n",
        "    #We build a probability model that will predict our testing data\n",
        "    probability_model = tf.keras.Sequential([self.model, tf.keras.layers.Softmax()])\n",
        "    predictions = probability_model.predict(testing_data)\n",
        "\n",
        "    scores = self.model.evaluate(testing_data,testing_labels)\n",
        "    print(\"Total tests: \", len(testing_labels))\n",
        "    #We check how many correct guesses the neural network made\n",
        "    self.loss = scores[0]*100\n",
        "    self.accuracy = scores[1]*100"
      ],
      "metadata": {
        "id": "zeYlLU1LJgMd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting Machine"
      ],
      "metadata": {
        "id": "tt40Ft1DrwfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientBoostingMachine(MachineLearningModel):\n",
        "    #Constructor\n",
        "    def __init__(self,name,verbose):\n",
        "        super(GradientBoostingMachine, self).__init__(model_name = name, verbose = verbose)\n",
        "\n",
        "    \"\"\"\n",
        "    def build(self, n_estimators = [1000], max_features = ['auto', 'sqrt', 'log2'], max_depth = [1, 3], subsample = [0.5, 1], learning_rate = [0.1]):\n",
        "        self.param_grid = {\n",
        "            'n_estimators'  : n_estimators,\n",
        "            'max_features'  : max_features,\n",
        "            'max_depth'     : max_depth,\n",
        "            'subsample'     : subsample,\n",
        "            'learning_rate' : learning_rate\n",
        "        }\n",
        "\n",
        "        self.grid = GridSearchCV(\n",
        "            estimator  = GradientBoostingClassifier(random_state=123),\n",
        "            param_grid = self.param_grid,\n",
        "            scoring    = 'accuracy',\n",
        "            n_jobs     = multiprocessing.cpu_count() - 1,\n",
        "            cv         = RepeatedKFold(n_splits=3, n_repeats=1, random_state=123),\n",
        "            refit      = True,\n",
        "            verbose    = self.verbose,\n",
        "            return_train_score = True\n",
        "        )\"\"\"\n",
        "\n",
        "    def build(self, n_estimators = 1000, max_features = 'auto', max_depth = 3 , subsample = 1,learning_rate = 0.1):\n",
        "        self.model  = GradientBoostingClassifier(learning_rate=learning_rate,n_estimators=n_estimators,subsample=subsample,max_depth=max_depth,max_features=max_features)\n",
        "\n",
        "    def compile(self):\n",
        "        self.build()\n",
        "    \n",
        "    #We compile the Gradient Boosting Machine\n",
        "    #def compile(self):\n",
        "       # self.build()\n",
        "\n",
        "    def train(self, training_data , training_labels):\n",
        "        self.model.fit(X = training_data, y = training_labels)\n",
        "\n",
        "    \"\"\"\n",
        "    def train(self, training_data , training_labels):\n",
        "        self.grid.fit(X = training_data, y = training_labels)\n",
        "        # Resultados\n",
        "        # ==============================================================================\n",
        "        resultados = pd.DataFrame(self.grid.cv_results_)\n",
        "        resultados.filter(regex = '(param*|mean_t|std_t)') \\\n",
        "            .drop(columns = 'params') \\\n",
        "            .sort_values('mean_test_score', ascending = False) \\\n",
        "            .head(4)\n",
        "\n",
        "        # Mejores hiperparámetros por validación cruzada\n",
        "        # ==============================================================================\n",
        "        print(\"----------------------------------------\")\n",
        "        print(\"Mejores hiperparámetros encontrados (cv)\")\n",
        "        print(\"----------------------------------------\")\n",
        "        print(self.grid.best_params_, \":\", self.grid.best_score_, self.grid.scoring)\n",
        "\n",
        "        self.modelo_final = self.grid.best_estimator_\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "    def evaluate(self, samples):\n",
        "        # Error de test del modelo final\n",
        "        # ==============================================================================\n",
        "        y_test = samples['testing_labels']\n",
        "        x_test = samples['testing_data']\n",
        "\n",
        "        self.predicciones = self.model.predict(X = x_test)\n",
        "        #self.predicciones[:10]\n",
        "\n",
        "        self.mat_confusion = confusion_matrix(\n",
        "            y_true    = y_test,\n",
        "            y_pred    = self.predicciones\n",
        "        )\n",
        "\n",
        "        self.accuracy = accuracy_score(\n",
        "            y_true    = y_test,\n",
        "            y_pred    = self.predicciones,\n",
        "            normalize = True\n",
        "        )\n",
        "        print(\"\\nGradient Boosting machine\\n\")\n",
        "        print(\"Confusion Matrix\")\n",
        "        print(\"-------------------\")\n",
        "        print(self.mat_confusion)\n",
        "        print(\"\")\n",
        "        self.accuracy  = self.accuracy *100\n",
        "        print(f\"Test Acurracy: {self.accuracy } %\\n\")"
      ],
      "metadata": {
        "id": "yVSOLI6wrzMP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our Dataparser class:"
      ],
      "metadata": {
        "id": "5PpEVlUPCcXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas\n",
        "\n",
        "#This class will process a dataset to be usable with our models\n",
        "class DataParser:\n",
        "  #We store the dataset properties during the construction phase\n",
        "  def __init__(self, data_src):\n",
        "    #If the dataset is a file, we store the filepath and the formatting:\n",
        "    if 'src' in data_src:\n",
        "      self.data_src = data_src['src']\n",
        "      self.sep = data_src['sep']\n",
        "\n",
        "    self.testing_samples = None\n",
        "\n",
        "  #This method will return the total outputs needed for our model to properly classify.\n",
        "  #Ex: If there are 3 unique classes, it will return 3, etc.\n",
        "  def get_unique_labels(self):\n",
        "    labels_set = set()\n",
        "    for i in range(0, len(self.labels)):\n",
        "      if self.labels[i] not in labels_set:\n",
        "        labels_set.add(self.labels[i])\n",
        "    if len(labels_set) <= 2:\n",
        "      return 1\n",
        "    return len(labels_set)\n",
        "\n",
        "  #Method to distribute and preprocess data\n",
        "  def preprocess(self, *, training, testing):\n",
        "    #Load our dataset into dataframe from source path\n",
        "    our_dataframe = pandas.read_csv(self.data_src, sep = self.sep)\n",
        "\n",
        "    #The labels are contained within the dataset so we grab them\n",
        "    self.labels = our_dataframe[\"legitimate\"]\n",
        "\n",
        "    #Delete useless features. We delete 'legitimate' because we are going to store it in labels and not the dataset\n",
        "    del our_dataframe[\"Name\"]\n",
        "    del our_dataframe[\"md5\"]\n",
        "    del our_dataframe[\"legitimate\"]\n",
        "\n",
        "    #We store the raw dataset\n",
        "    self.raw_dataset = our_dataframe\n",
        "\n",
        "    #We normalize dataset to values from 0 to 1. We divide each column value by the largest value in that column:\n",
        "    for column in self.raw_dataset:\n",
        "      LARGEST_VAL_FOR_COL = np.argmax(self.raw_dataset[column])\n",
        "      self.raw_dataset[column] = self.raw_dataset[column] / self.raw_dataset[column][LARGEST_VAL_FOR_COL]\n",
        "\n",
        "    #We transform our data into numpy\n",
        "    dataset_as_array = self.raw_dataset.to_numpy()\n",
        "    self.labels = self.labels.to_numpy()\n",
        "\n",
        "    #We split our dataset into training and testing. These are the data we use for training/validating the model\n",
        "    self.training_data, self.testing_data, self.training_labels, self.testing_labels = train_test_split(dataset_as_array, self.labels, test_size = 0.2)\n",
        "\n",
        "    #We store the following properties so that they may be used later:\n",
        "    self.input_features = self.training_data[0].shape\n",
        "    self.unique_labels = self.get_unique_labels()\n",
        "\n",
        "  #Method to print dataset information\n",
        "  def print_dataset_info(self):\n",
        "    print(\"Dataset properties: \")\n",
        "    print(\"\\tSize: \", len(self.data_src[0]))\n",
        "    print(\"\\tParameters:\", len(self.data_src))\n",
        "\n",
        "  #We create testing samples from the testing data. We COULD use the entire testing data, and thatd be fine, but we choose not to\n",
        "  def generate_testing_samples(self, *, sample_size):\n",
        "    #If we have already created sample data, we don't do it again\n",
        "    if self.testing_samples == None:\n",
        "      samples = {}\n",
        "      testing_labels = []\n",
        "      testing_data = []\n",
        "      #We generate our samples: (Currently the possibility exists that we'll generate the same data point numerous times. We will fix in the future)\n",
        "      generated_nums = {}\n",
        "      for i in range(0, min(sample_size, len(self.testing_labels))):\n",
        "        random_index = randint(0, min(sample_size, len(self.testing_labels)))\n",
        "        testing_labels.append(self.testing_labels[random_index])\n",
        "        testing_data.append(self.testing_data[random_index])\n",
        "\n",
        "      #We package our samples into a dictionary containing two NP arrays(MUST BE NP ARRAY OR SYSTEM WILL BREAK)\n",
        "      testing_labels = np.array(testing_labels)\n",
        "      testing_data = np.array(testing_data)\n",
        "      samples['testing_labels'] = testing_labels\n",
        "      samples['testing_data'] = testing_data\n",
        "\n",
        "      #We assign our samples\n",
        "      self.testing_samples = samples\n",
        "    return self.testing_samples\n"
      ],
      "metadata": {
        "id": "jXSqsM6ECaty"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following segment shows us how to use the dataparser with neural networks. The same process holds more or less for other models:"
      ],
      "metadata": {
        "id": "VIUkgDy9TKsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We create an instance of our data parser\n",
        "#This dictionary will contain the properties of our dataset:\n",
        "dataset_information = {\n",
        "    #SELECT HERE THE PATH FOR THE MALWARE DATASET:\n",
        "    'src': '/content/MalwareData.csv',\n",
        "    #Do not change sep, the only thing you should change is the 'src' key to wherever your Malware dataset file is\n",
        "    'sep': '|'\n",
        "}\n",
        "our_parser = DataParser(dataset_information)\n",
        "\n",
        "#Parser creates training and testing dataset from original dataset\n",
        "our_parser.preprocess(training = 0.8, testing = 0.2)\n",
        "\n",
        "#We define our NN hyperparameters and instantiate:\n",
        "#Note: input_size / our_parser.input_features gives us the number of parameters for each malware entry(In this case around 54)\n",
        "#Note: output_size / our_parser.unique_labels gives us the number of output classes(In this case 1: Legitimate 0: Malware)\n",
        "Malware_SNN_1 = NeuralNetwork(\n",
        "    architecture = [], \n",
        "    #Dataparser gives us methods to automatically calculate the input features and the unique class labels:\n",
        "    input_size = our_parser.input_features, \n",
        "    output_size = our_parser.unique_labels, \n",
        "    epochs = 3, \n",
        "    name = \"Linear Neural Network\", \n",
        "    verbose = False)\n",
        "\n",
        "Malware_GBM = GradientBoostingMachine(name=\"Malware Gradient Boosting Machine\",verbose= True);\n",
        "\n",
        "#We create another neural network to experiment with:\n",
        "Malware_SNN_2 = NeuralNetwork(\n",
        "    architecture = [64, 128], \n",
        "    input_size = our_parser.input_features, \n",
        "    output_size = our_parser.unique_labels, \n",
        "    epochs = 2, \n",
        "    name = \"Nonlinear Neural Network\", \n",
        "    verbose = False)\n",
        "\n",
        "#We create an array of models that contains the 2 neural networks we created above\n",
        "models_array  = []\n",
        "models_array.append(Malware_SNN_1)\n",
        "models_array.append(Malware_GBM)\n",
        "\n",
        "#We compile our models:\n",
        "for model in models_array:\n",
        "  model.compile()\n",
        "\n",
        "#We train our models:\n",
        "for model in models_array:\n",
        "  model.train(\n",
        "      training_data = our_parser.training_data, \n",
        "      training_labels = our_parser.training_labels\n",
        "  )\n",
        "\n",
        "#We determine their accuracy given testing samples\n",
        "for model in models_array:\n",
        "  model.evaluate(our_parser.generate_testing_samples(sample_size = 100))\n",
        "\n",
        "for model in models_array:\n",
        "  print(\"Model name: \", model.name, \"Accuracy: \", model.accuracy)\n"
      ],
      "metadata": {
        "id": "CBHJfADQR3E0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a82b9126-ff51-4360-9b2a-f0b2ecff06b0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "2416/2416 [==============================] - 3s 1ms/step - loss: 0.4326 - accuracy: 0.7979 - val_loss: 0.3111 - val_accuracy: 0.8880\n",
            "Epoch 2/3\n",
            "2416/2416 [==============================] - 4s 2ms/step - loss: 0.2566 - accuracy: 0.9122 - val_loss: 0.2232 - val_accuracy: 0.9208\n",
            "Epoch 3/3\n",
            "2416/2416 [==============================] - 4s 1ms/step - loss: 0.1985 - accuracy: 0.9310 - val_loss: 0.1855 - val_accuracy: 0.9343\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.2224 - accuracy: 0.9100\n",
            "Total tests:  100\n",
            "\n",
            "Gradient Boosting machine\n",
            "\n",
            "Confusion Matrix\n",
            "-------------------\n",
            "[[68  0]\n",
            " [ 0 32]]\n",
            "\n",
            "Test Acurracy: 100.0 %\n",
            "\n",
            "Model name:  Linear Neural Network Accuracy:  91.00000262260437\n",
            "Model name:  Malware Gradient Boosting Machine Accuracy:  100.0\n"
          ]
        }
      ]
    }
  ]
}