import sys
import tkinter as tk
from tkinter import filedialog, ttk
from PIL import ImageTk, Image
from threading import *
import random
import time

# Machine learning:
import tensorflow as tf
# C math port
import numpy as np
# Random integer  generator
from random import randint

WIDTH_WINDOW = 900
HEIGHT_WINDOW = 475
flag_dataset = False
flag_compilado = False
flag_entrenado = False
flag_evaluacion = False
folder_selected = ""

global menu
global our_parser
global ai_models


# Abstract class for machine learning models. We will use this to inherit to specific models and override each method when inherited.
class MachineLearningModel:
    # CONSTRUCTOR: Currently does nothing besides storing the name we gave it
    def __init__(self, *, model_name, verbose):
        self.name = model_name
        self.verbose = verbose
        self.training_time = 0
        self.evaluation_time = 0

    # Method for compiling our machine learning model
    def compile(self):
        if self.verbose:
            print("Model Compiled!")

    # Method for training our machine learning model. It will use the processed dataset
    def train(self, data):
        if self.verbose:
            print("Model Trained!")
        self.training_time = 1000

    # Method for evaluating our machine learning model. We use this to compute how accurate it is
    def evaluate(self, samples):
        for sample in samples:
            print(sample)
        self.precision = 0.9
        self.loss = 0.1
        if self.verbose:
            print("Model Accuracy:", self.precision)
        self.mean_evaluation_time = 10

    # Method to run the entire model. This is interface that would be used for any model.
    def run_model(self, dataset, samples):
        self.compile()
        self.train(dataset)
        self.evaluate(samples)


class ConvolutionalNeuralNetwork(MachineLearningModel):
    def build(self):
        layer_array = []
        # Input Layer
        layer_array.append(
            tf.keras.layers.Conv2D(
                28,
                (3, 3),
                activation='relu',
                input_shape=self.network_architecture['input_shape']
            )
        )
        # layer_array.append(tf.keras.layers.MaxPooling2D((2, 2)))

        # Hidden Layers
        for layer in self.network_architecture['architecture']:
            layer_array.append(tf.keras.layers.Conv2D(layer, (3, 3), activation='relu'))
            layer_array.append(tf.keras.layers.MaxPooling2D((2, 2)))

        # Output Layers
        layer_array.append(tf.keras.layers.Flatten())
        layer_array.append(tf.keras.layers.Dense(64, activation="relu"))
        layer_array.append(tf.keras.layers.Dense(self.network_architecture['output_size']))

        # Our complete model
        self.model = tf.keras.Sequential(layer_array)

    def __init__(self, *, architecture, input_size, input_shape, output_size, epochs, name, verbose):
        # We inherit from the machine learning model class:
        super(ConvolutionalNeuralNetwork, self).__init__(model_name=name, verbose=verbose)
        # We create a dictionary/hash map with the properties of our convolutional neural network
        self.network_architecture = {'architecture': architecture,
                                     'input_size': input_size,
                                     'input_shape': input_shape,
                                     'output_size': output_size,
                                     'epochs': epochs
                                     }
        # We build our neural network based on the specifications
        self.build()

    # Compiling model with hard-coded metrics
    def compile(self):
        our_optimizer = 'adam'
        our_metric = 'accuracy'
        self.model.compile(optimizer=our_optimizer,
                           loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                           metrics=[our_metric])

    # Train model
    def train(self, *, training_data, training_labels):
        start = time.time()
        self.model.fit(training_data, training_labels, epochs=self.network_architecture['epochs'])
        end = time.time()
        self.training_time = end - start

    # Evaluate the accuracy of the model based on testing samples
    def evaluate(self, samples):
        # We grab the testing data so we can run our test

        start = time.time()

        testing_labels = samples['testing_labels']
        testing_data = samples['testing_data']

        # We build a probability model that will predict our testing data
        probability_model = tf.keras.Sequential([self.model, tf.keras.layers.Softmax()])
        predictions = probability_model.predict(testing_data)

        # We check how many correct guesses the neural network made
        correct_guesses = 0
        for i in range(0, len(testing_labels)):
            if testing_labels[i] == np.argmax(probability_model.predict(np.expand_dims(testing_data[i], 0))):
                correct_guesses += 1
        self.accuracy = correct_guesses / len(testing_data)

        end = time.time()
        self.evaluation_time = end - start


class NeuralNetwork(MachineLearningModel):
    # Not inherited: Method to build neural network based on architecture specifications
    def build(self):
        layer_array = []
        # We attach our input layer into our layer array:
        layer_array.append(tf.keras.layers.Flatten(input_shape=self.network_architecture['input_size']))
        # We attach our hidden layers into our layer array:
        for layer in self.network_architecture['architecture']:
            layer_array.append(tf.keras.layers.Dense(layer, activation="relu"))
        # We attach our output layer into our layer array
        layer_array.append(tf.keras.layers.Dense(self.network_architecture['output_size']))
        # We create a neural network model using the layer array specifications. self.model is our actual network
        self.model = tf.keras.Sequential(layer_array)

    # Constructor:
    def __init__(self, *, architecture, input_size, output_size, epochs, name, verbose):
        # We inherit from the machine learning model class:
        super(NeuralNetwork, self).__init__(model_name=name, verbose=verbose)
        # We create a dictionary/hash map with the properties of our neural network
        self.network_architecture = {'architecture': architecture,
                                     'input_size': input_size,
                                     'output_size': output_size,
                                     'epochs': epochs}
        # We build our neural network based on the specifications
        self.build()

    # We compile the built neural network
    def compile(self):
        our_optimizer = 'adam'
        our_metric = 'accuracy'
        self.model.compile(optimizer=our_optimizer,
                           loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[our_metric])

    # We train our neural network with a given dataset
    def train(self, *, training_data, training_labels):
        start = time.time()
        self.model.fit(training_data, training_labels, epochs=self.network_architecture['epochs'])
        end = time.time()
        self.training_time = end - start

    # We check how accurate our neural network is
    def evaluate(self, samples):
        # We grab the testing data so we can run our test
        start = time.time()

        testing_labels = samples['testing_labels']
        testing_data = samples['testing_data']

        # We build a probability model that will predict our testing data
        probability_model = tf.keras.Sequential([self.model, tf.keras.layers.Softmax()])
        predictions = probability_model.predict(testing_data)

        # We check how many correct guesses the neural network made
        correct_guesses = 0
        for i in range(0, len(testing_labels)):
            if testing_labels[i] == np.argmax(probability_model.predict(np.expand_dims(testing_data[i], 0))):
                correct_guesses += 1
        self.accuracy = correct_guesses / len(testing_data)

        end = time.time()
        self.evaluation_time = end - start


# This class will process data and adapt it to something a given model can work with
# VERY UNFINISHED!!! Currently has minimum requirements for neural network compatibility
class DataParser:
    # We store the dataset during the construction phase
    def __init__(self, data_src):
        self.data_src = data_src
        self.neural_network_dataset = None
        self.testing_samples = None

    # We will use this method to compute the number of unique labels. Currently not functional!!!
    def get_unique_labels(self):
        return 10

    # Method to distribute and preprocess data
    def preprocess(self, *, training, testing):
        self.training_size = training
        self.testing_size = testing
        (self.training_data, self.training_labels), (self.testing_data, self.testing_labels) = self.data_src.load_data()
        self.training_data = self.training_data / 255
        self.testing_data = self.testing_data / 255
        self.data_shape = self.training_data[0].shape
        self.output_shape = self.get_unique_labels()

    # Method to print dataset information
    def print_dataset_info(self):
        print("Dataset properties: ")
        print("\tSize: ", len(self.data_src[0]))
        print("\tParameters:", len(self.data_src))

    # We create testing samples from the testing data. We COULD use the entire testing data, and thatd be fine, but we choose not to
    def generate_testing_samples(self, *, sample_size):
        # If we have already created sample data, we don't do it again
        if self.testing_samples == None:
            samples = {}
            testing_labels = []
            testing_data = []
            # We generate our samples: (Currently the possibility exists that we'll generate the same data point numerous times. We will fix in the future)
            for i in range(0, min(sample_size, len(self.testing_labels))):
                random_index = randint(0, min(sample_size, len(self.testing_labels)))
                testing_labels.append(self.testing_labels[random_index])
                testing_data.append(self.testing_data[random_index])

            # We package our samples into a dictionary containing two NP arrays(MUST BE NP ARRAY OR SYSTEM WILL BREAK)
            testing_labels = np.array(testing_labels)
            testing_data = np.array(testing_data)
            samples['testing_labels'] = testing_labels
            samples['testing_data'] = testing_data

            # We assign our samples
            self.testing_samples = samples
        return self.testing_samples

    # We convert our dataset into one compatible with a sequential neural network
    def convert_to_neural_network(self):
        if self.neural_network_dataset == None:
            self.neural_network_dataset = self.data_src
        return self.neural_network_dataset

    # Future implementations
    def convert_to_cnn(self):
        pass

    def convert_to_kernel(self):
        pass

    def convert_to_svm(self):
        pass

    def convert_to_gradient_ascent(self):
        pass

    def convert_to_gradient_descent(self):
        pass


class App(ttk.Frame):
    global WIDTH_WINDOW, HEIGHT_WINDOW, flag_dataset, flag_compilado, flag_entrenado, folder_selected

    green_check_images = [None, None, None]
    main_window = tk.Tk()

    def __init__(self, title, color):
        super().__init__(self.main_window)
        self.main_window.title(title)
        self.main_window.configure(width=WIDTH_WINDOW, height=HEIGHT_WINDOW, bg=color)
        self.folder_selected = folder_selected

    def make_label(self, x, y, w, h, *args, **kwargs):
        frame = ttk.Frame(self.main_window, height=h, width=w)
        frame.pack_propagate(True)
        frame.place(x=x, y=y)
        label = ttk.Label(frame, *args, **kwargs, wraplength=w)
        label.pack(fill=tk.BOTH, expand=False)
        return label

    def make_button(self, x, y, w, h, button_text, func=exit, *args, **kwargs):
        button = ttk.Button(self.main_window, text=button_text, command=func, *args, **kwargs)
        button.place(x=x, y=y, width=w, height=h)
        return button

    def make_image(self, x, y, image):
        image = Image.open(image)
        render = ImageTk.PhotoImage(image)
        img = ttk.Label(self.main_window, image=render)
        img.image = render
        img.place(x=x, y=y)
        return img

    def exit(self):
        self.main_window.destroy()

    def check_steps(self):
        global flag_dataset, flag_compilado, flag_entrenado

        if flag_dataset and isinstance(self.green_check_images[0], type(None)):
            self.green_check_images[0] = self.make_image(350, 153, "./Images/check_small.png")
        if flag_entrenado and flag_compilado and isinstance(self.green_check_images[1], type(None)):
            self.green_check_images[1] = self.make_image(350, 203, "./Images/check_small.png")
        if flag_evaluacion and isinstance(self.green_check_images[2], type(None)):
            self.green_check_images[2] = self.make_image(350, 253, "./Images/check_small.png")

        if not flag_dataset and not isinstance(self.green_check_images[0], type(None)):
            self.green_check_images[0].destroy()
            self.green_check_images[0] = None
        if not flag_entrenado and not flag_compilado and not isinstance(self.green_check_images[1], type(None)):
            self.green_check_images[1].destroy()
            self.green_check_images[1] = None
        if not flag_evaluacion and not isinstance(self.green_check_images[2], type(None)):
            self.green_check_images[2].destroy()
            self.green_check_images[2] = None

        # print(str(flag_dataset) + "  -  " + str(flag_compilado) + "  -  " + str(flag_entrenado))


class Main_window(App):

    def __init__(self, title, color):
        App.__init__(self, title, color)
        self.add_widgets()

    def window_load_dataset_show(self):
        global folder_selected, flag_dataset, flag_entrenado, flag_compilado, flag_evaluacion, menu, our_parser

        folder_selected = ""
        flag_dataset = False
        flag_compilado = False
        flag_entrenado = False
        flag_evaluacion = False

        # We create an instance of our data parser, we will use the MNIST dataset to test
        our_parser = DataParser(tf.keras.datasets.mnist)
        if our_parser:
            # Parser creates training and testing dataset from original dataset
            our_parser.preprocess(training=0.7, testing=0.3)
            flag_dataset = True
        else:
            our_parser = None
            flag_dataset = False

        menu.check_steps()

    def window_compile_train_show(self):
        global flag_dataset, flag_entrenado, flag_compilado, menu

        if flag_dataset:
            window = Compile_train_window("Compilar y Entrenar", "#ECECEC")
            menu.check_steps()
            return window
        else:
            self.window_load_dataset_show()
            if flag_dataset:
                window = Compile_train_window("Compilar y Entrenar", "#ECECEC")
                menu.check_steps()
                return window
        return

    def window_evaluate_show(self):
        global flag_dataset, flag_entrenado, flag_compilado, menu

        if flag_compilado and flag_entrenado:
            window = Evaluate_window("Evaluar", "#ECECEC")
            menu.check_steps()
            return window
        else:
            self.window_compile_train_show()

    def restablecer_parametros_iniciales(self):
        global folder_selected, flag_dataset, flag_entrenado, flag_compilado, flag_evaluacion, menu
        folder_selected = ""
        flag_dataset = False
        flag_compilado = False
        flag_entrenado = False
        flag_evaluacion = False
        menu.check_steps()

    def add_widgets(self):
        self.make_label(40, 40, 300, 100, text='Clasificación de Malware Mediante Deep Learning', background='white',
                        font=("Arial", 19))
        self.make_button(40, 150, 300, 40, "Cargar Dataset", self.window_load_dataset_show)
        self.make_button(40, 200, 300, 40, "Compilar y Entrenar", self.window_compile_train_show)
        self.make_button(40, 250, 300, 40, "Evaluar", self.window_evaluate_show)
        self.make_button(40, 400, 100, 40, "Salir", self.exit)
        self.make_button(150, 400, 190, 40, "Restablecer", self.restablecer_parametros_iniciales)
        self.make_image(500, 60, "./Images/uaslp.jpg")


class Secondary_window(App):
    global WIDTH_WINDOW, HEIGHT_WINDOW, flag_dataset, flag_compilado, flag_entrenado, folder_selected

    def __init__(self, title, color):
        self.main_window = tk.Toplevel()
        App.__init__(self, title, color)

    def exit(self):
        self.main_window.withdraw()


class Compile_train_window(Secondary_window):
    model_progresses_compile = []
    model_progresses_train = []

    threads_compile = []
    threads_train = []

    def __init__(self, title, color):
        Secondary_window.__init__(self, title, color)
        self.provisional_height = HEIGHT_WINDOW + 40 * 2
        self.main_window.configure(width=WIDTH_WINDOW, height=self.provisional_height, bg=color)
        self.model_progresses_compile = []
        self.model_progresses_train = []
        self.threads_compile = []
        self.threads_train = []
        self.load_widgets()

    def load_widgets(self):
        self.make_label(40, 40, WIDTH_WINDOW - 100, 50,
                        text="Estatus", background='white',
                        font="Arial 12 bold")

        self.make_button(WIDTH_WINDOW - 150, self.provisional_height - 65, 100, 40, "Salir",
                         self.exit)
        self.make_button(WIDTH_WINDOW - 250, 40 + 15, 200, 40, "Compilar y Entrenar",
                         func=lambda: self.show_progress(40, 120, self.model_progresses_compile,
                                                         self.threads_compile, "compile"))

    def show_progress(self, x, y, model_processes, threads, proccess):
        global flag_dataset, flag_entrenado, flag_compilado, flag_evaluacion, menu, our_parser, ai_models

        # We define our CNN hyperparameters and instantiate:
        our_CNN = ConvolutionalNeuralNetwork(
            architecture=[28 * 2, 28 * 4, 28 * 2],
            input_size=10,
            input_shape=(28, 28, 1),
            output_size=our_parser.output_shape,
            epochs=1,
            name="Our First CNN",
            verbose=False)

        # We define our NN hyperparameters and instantiate:
        mnist_network = NeuralNetwork(
            architecture=[128, 64],
            input_size=our_parser.data_shape,
            output_size=our_parser.output_shape,
            epochs=3,
            name="MNIST neural network",
            verbose=False)

        # We create an array of models that contains the 2 neural networks we created above
        ai_models = []
        ai_models.append(our_CNN)
        ai_models.append(mnist_network)

        for model in ai_models:
            model.compile()

        if (proccess == "train") or (
                proccess == "compile"):
            for i in range(0, len(ai_models)):
                model_processes.append(
                    ttk.Progressbar(self.main_window, orient=tk.HORIZONTAL, length=WIDTH_WINDOW - 100,
                                    mode='determinate'))
                model_processes[i].place(x=x, y=y + 70 * i)
                self.make_label(x, (y - 30) + 70 * i, WIDTH_WINDOW - 100, 50,
                                text=ai_models[i], background='white',
                                font=("Arial", 11))

            self.main_window.update()
            i = 0
            for model in ai_models:
                model.train(
                    training_data=our_parser.training_data,
                    training_labels=our_parser.training_labels
                )
                thread = (Thread(target=self.step, args=(i, model_processes)))
                threads.append(thread)
                i = i + 1

            for j in threads:
                j.start()


            flag_dataset = True
            flag_compilado = True
            flag_entrenado = True
            flag_evaluacion = False
            self.make_image(160, 35, "./Images/check_small.png")

            menu.check_steps()

    def step(self, i, model_processes):
        try:
            while model_processes[i]['value'] < 99:
                model_processes[i]["value"] += 1 * random.randint(0, 9)
                self.main_window.update_idletasks()
                time.sleep(.05 * random.randint(0, 9))
        except:
            x = [i for i, a in locals().items() if a == model_processes][0]
            print("Bad Execcution, Thread " + str(i) + " Stopped (" + x + ")")


class Evaluate_window(Secondary_window):

    def __init__(self, title, color):
        Secondary_window.__init__(self, title, color)
        self.provisional_height = HEIGHT_WINDOW + 290
        self.provisional_width = WIDTH_WINDOW + 200 * 2
        self.main_window.configure(height=self.provisional_height, width=self.provisional_width,
                                   bg=color)
        self.load_widgets()

    def load_widgets(self):


        global menu, flag_evaluacion, ai_models

        # We determine their accuracy given testing samples
        for model in ai_models:
            model.evaluate(our_parser.generate_testing_samples(sample_size=300))

        self.make_label(40, 40, WIDTH_WINDOW - 100, 50,
                        text="Evaluación", background='white',
                        font="Arial 12 bold")

        self.make_button(self.provisional_width - 130, self.provisional_height - 65, 100, 40, "Salir",
                         self.exit)

        for i in range(0, len(ai_models)):
            self.make_label(500 * i + 50, 120, self.provisional_width - 100, 50,
                            text=ai_models[i].name, background='white',
                            font="Arial 12 bold")
            # self.make_image(500 * i + 50, 150, "./Images/graph.png")
            self.make_label(500 * i + 50, 480, self.provisional_width - 100, 50,
                            text="Tiempo de Entrenamiento: " + str(ai_models[i].training_time) + " seg",
                            background='white',
                            font="Arial 11 bold")

            self.make_label(500 * i + 50, 530, self.provisional_width - 100, 50,
                            text="Tiempo de Evaluación: " + str(ai_models[i].evaluation_time) + " seg", background='white',
                            font="Arial 11 bold")

            self.make_label(500 * i + 50, 580, self.provisional_width - 100, 50,
                            text="Precisión del Modelo: " + str(ai_models[i].accuracy), background='white',
                            font="Arial 11 bold")

            self.make_label(500 * i + 50, 630, self.provisional_width - 100, 50,
                            text="Memoria consumida: " + str(sys.getsizeof(ai_models[i])) + " MB", background='white',
                            font="Arial 11 bold")

        flag_evaluacion = True
        menu.check_steps()


def main():
    global menu
    menu = Main_window("Clasificación de Malware Mediante Deep Learning", "#ECECEC")
    menu.mainloop()


if __name__ == '__main__':
    main()
