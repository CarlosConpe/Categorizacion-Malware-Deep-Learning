from sklearn.model_selection import train_test_split
import numpy as np
from random import randint
import pandas

#This class will process a dataset to be usable with our models
class DataParser:
  #We store the dataset properties during the construction phase
  def __init__(self, data_src):
    #If the dataset is a file, we store the filepath and the formatting:
    if 'src' in data_src:
      self.data_src = data_src['src']
      self.sep = data_src['sep']

    self.testing_samples = None

  #This method will return the total outputs needed for our model to properly classify.
  #Ex: If there are 3 unique classes, it will return 3, etc.
  def get_unique_labels(self):
    labels_set = set()
    for i in range(0, len(self.labels)):
      if self.labels[i] not in labels_set:
        labels_set.add(self.labels[i])
    if len(labels_set) <= 2:
      return 1
    return len(labels_set)

  #Method to distribute and preprocess data
  def preprocess(self, *, training, testing):
    #Load our dataset into dataframe from source path
    our_dataframe = pandas.read_csv(self.data_src, sep = self.sep)

    #The labels are contained within the dataset so we grab them
    self.labels = our_dataframe["legitimate"]

    #Delete useless features. We delete 'legitimate' because we are going to store it in labels and not the dataset
    del our_dataframe["Name"]
    del our_dataframe["md5"]
    del our_dataframe["legitimate"]

    #We store the raw dataset
    self.raw_dataset = our_dataframe

    #We normalize dataset to values from 0 to 1. We divide each column value by the largest value in that column:
    for column in self.raw_dataset:
      LARGEST_VAL_FOR_COL = np.argmax(self.raw_dataset[column])
      self.raw_dataset[column] = self.raw_dataset[column] / self.raw_dataset[column][LARGEST_VAL_FOR_COL]

    #We transform our data into numpy
    dataset_as_array = self.raw_dataset.to_numpy()
    self.labels = self.labels.to_numpy()

    #We split our dataset into training and testing. These are the data we use for training/validating the model
    self.training_data, self.testing_data, self.training_labels, self.testing_labels = train_test_split(dataset_as_array, self.labels, test_size = testing)

    #We store the following properties so that they may be used later:
    self.input_features = self.training_data[0].shape
    self.unique_labels = self.get_unique_labels()

  #Method to print dataset information
  def print_dataset_info(self):
    print("Dataset properties: ")
    print("\tSize: ", len(self.data_src[0]))
    print("\tParameters:", len(self.data_src))

  #We create testing samples from the testing data. We COULD use the entire testing data, and thatd be fine, but we choose not to
  def generate_testing_samples(self, *, sample_size):
    #If we have already created sample data, we don't do it again
    if self.testing_samples == None:
      samples = {}
      testing_labels = []
      testing_data = []
      #We generate our samples: (Currently the possibility exists that we'll generate the same data point numerous times. We will fix in the future)
      generated_nums = {}
      for i in range(0, min(sample_size, len(self.testing_labels))):
        random_index = randint(0, min(sample_size, len(self.testing_labels)))
        testing_labels.append(self.testing_labels[random_index])
        testing_data.append(self.testing_data[random_index])

      #We package our samples into a dictionary containing two NP arrays(MUST BE NP ARRAY OR SYSTEM WILL BREAK)
      testing_labels = np.array(testing_labels)
      testing_data = np.array(testing_data)
      samples['testing_labels'] = testing_labels
      samples['testing_data'] = testing_data

      #We assign our samples
      self.testing_samples = samples
    return self.testing_samples