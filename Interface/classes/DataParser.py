# Random integer  generator
from random import randint
import numpy as np

# This class will process data and adapt it to something a given model can work with
# VERY UNFINISHED!!! Currently has minimum requirements for neural network compatibility
class DataParser:
    # We store the dataset during the construction phase
    def __init__(self, data_src):
        self.data_src = data_src
        self.neural_network_dataset = None
        self.testing_samples = None

    # We will use this method to compute the number of unique labels. Currently not functional!!!
    def get_unique_labels(self):
        return 10

    # Method to distribute and preprocess data
    def preprocess(self, *, training, testing):
        self.training_size = training
        self.testing_size = testing
        (self.training_data, self.training_labels), (
            self.testing_data,
            self.testing_labels,
        ) = self.data_src.load_data()
        self.training_data = self.training_data / 255
        self.testing_data = self.testing_data / 255
        self.data_shape = self.training_data[0].shape
        self.output_shape = self.get_unique_labels()

    # Method to print dataset information
    def print_dataset_info(self):
        print("Dataset properties: ")
        print("\tSize: ", len(self.data_src[0]))
        print("\tParameters:", len(self.data_src))

    # We create testing samples from the testing data. We COULD use the entire testing data, and thatd be fine, but we choose not to
    def generate_testing_samples(self, *, sample_size):
        # If we have already created sample data, we don't do it again
        if self.testing_samples == None:
            samples = {}
            testing_labels = []
            testing_data = []
            # We generate our samples: (Currently the possibility exists that we'll generate the same data point numerous times. We will fix in the future)
            for i in range(0, min(sample_size, len(self.testing_labels))):
                random_index = randint(0, min(sample_size, len(self.testing_labels)))
                testing_labels.append(self.testing_labels[random_index])
                testing_data.append(self.testing_data[random_index])

            # We package our samples into a dictionary containing two NP arrays(MUST BE NP ARRAY OR SYSTEM WILL BREAK)
            testing_labels = np.array(testing_labels)
            testing_data = np.array(testing_data)
            samples["testing_labels"] = testing_labels
            samples["testing_data"] = testing_data

            # We assign our samples
            self.testing_samples = samples
        return self.testing_samples

    # We convert our dataset into one compatible with a sequential neural network
    def convert_to_neural_network(self):
        if self.neural_network_dataset == None:
            self.neural_network_dataset = self.data_src
        return self.neural_network_dataset

    # Future implementations
    def convert_to_cnn(self):
        pass

    def convert_to_kernel(self):
        pass

    def convert_to_svm(self):
        pass

    def convert_to_gradient_ascent(self):
        pass

    def convert_to_gradient_descent(self):
        pass
